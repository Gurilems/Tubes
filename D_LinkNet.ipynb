{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPgvIEbdJppBtc+uw+qePQE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "455525c7c52748dcac1989af5ad1117d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2f0a9397a450448d8a1ef142c84ea99b",
              "IPY_MODEL_bf6d5d7d47ea4bde9fcacbd3a0d2bf3a",
              "IPY_MODEL_4f793702741141bf845bad72c9d9523a"
            ],
            "layout": "IPY_MODEL_1110f27d417247ab973ecb972350efc7"
          }
        },
        "2f0a9397a450448d8a1ef142c84ea99b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c574b5962a3449acb49c9c3e3e5e8dcb",
            "placeholder": "​",
            "style": "IPY_MODEL_72431c37d2a6486e9955ff17b4d99b05",
            "value": "100%"
          }
        },
        "bf6d5d7d47ea4bde9fcacbd3a0d2bf3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_00fd2b48028c4d1a95fcd8cb666fe749",
            "max": 87319819,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_008de073eeeb49828b98083074ed8009",
            "value": 87319819
          }
        },
        "4f793702741141bf845bad72c9d9523a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5a6384b1a5ba4ca4a1a11e0f4dca706a",
            "placeholder": "​",
            "style": "IPY_MODEL_e64249df81f14bdab2b83ec66c7b3dd8",
            "value": " 83.3M/83.3M [00:00&lt;00:00, 280MB/s]"
          }
        },
        "1110f27d417247ab973ecb972350efc7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c574b5962a3449acb49c9c3e3e5e8dcb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "72431c37d2a6486e9955ff17b4d99b05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "00fd2b48028c4d1a95fcd8cb666fe749": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "008de073eeeb49828b98083074ed8009": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5a6384b1a5ba4ca4a1a11e0f4dca706a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e64249df81f14bdab2b83ec66c7b3dd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gurilems/Tubes/blob/main/D_LinkNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XVc7twEH4wF6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as data\n",
        "from torch.autograd import Variable as V\n",
        "from torchvision import models\n",
        "import torch.nn.functional as F\n",
        "from google.colab import drive\n",
        "from functools import partial\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "from time import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "du1cnrpZNTGr"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6xgPaVK41CD",
        "outputId": "a08386ff-f5b5-47b8-9142-820d4c3c3f07"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MyFrame():\n",
        "    def __init__(self, net, loss, lr=2e-4, evalmode = False):\n",
        "        self.net = net().cuda()\n",
        "        self.net = torch.nn.DataParallel(self.net, device_ids=range(torch.cuda.device_count()))\n",
        "        self.optimizer = torch.optim.Adam(params=self.net.parameters(), lr=lr)\n",
        "        self.loss = loss()\n",
        "        self.old_lr = lr\n",
        "        if evalmode:\n",
        "            for i in self.net.modules():\n",
        "                if isinstance(i, nn.BatchNorm2d):\n",
        "                    i.eval()\n",
        "        \n",
        "    def set_input(self, img_batch, mask_batch=None, img_id=None):\n",
        "        self.img = img_batch.cuda()\n",
        "        if mask_batch is not None:\n",
        "            self.mask = mask_batch.cuda()\n",
        "        else:\n",
        "            self.mask = None\n",
        "        self.img_id = img_id\n",
        "        \n",
        "    def test_one_img(self, img):\n",
        "        pred = self.net.forward(img)\n",
        "        \n",
        "        pred[pred>0.5] = 1\n",
        "        pred[pred<=0.5] = 0\n",
        "\n",
        "        mask = pred.squeeze().cpu().data.numpy()\n",
        "        return mask\n",
        "    \n",
        "    def test_batch(self):\n",
        "        self.forward(volatile=True)\n",
        "        mask =  self.net.forward(self.img).cpu().data.numpy().squeeze(1)\n",
        "        mask[mask>0.5] = 1\n",
        "        mask[mask<=0.5] = 0\n",
        "        \n",
        "        return mask, self.img_id\n",
        "    \n",
        "    def test_one_img_from_path(self, path):\n",
        "        img = cv2.imread(path)\n",
        "        img = np.array(img, np.float32)/255.0 * 3.2 - 1.6\n",
        "        img = V(torch.Tensor(img).cuda())\n",
        "        \n",
        "        mask = self.net.forward(img).squeeze().cpu().data.numpy()#.squeeze(1)\n",
        "        mask[mask>0.5] = 1\n",
        "        mask[mask<=0.5] = 0\n",
        "        \n",
        "        return mask\n",
        "        \n",
        "    def forward(self, volatile=False):\n",
        "        self.img = V(self.img.cuda(), volatile=volatile)\n",
        "        if self.mask is not None:\n",
        "            self.mask = V(self.mask.cuda(), volatile=volatile)\n",
        "        \n",
        "    def optimize(self):\n",
        "        self.net.train()\n",
        "        self.optimizer.zero_grad()\n",
        "        output = self.net(self.img)\n",
        "        loss = self.loss(output, self.mask)\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        return loss.item()\n",
        "\n",
        "        \n",
        "    def save(self, path):\n",
        "        torch.save(self.net.state_dict(), path)\n",
        "        \n",
        "    def load(self, path):\n",
        "        self.net.load_state_dict(torch.load(path))\n",
        "    \n",
        "    def update_lr(self, new_lr, mylog, factor=False):\n",
        "        if factor:\n",
        "            new_lr = self.old_lr / new_lr\n",
        "        for param_group in self.optimizer.param_groups:\n",
        "            param_group['lr'] = new_lr\n",
        "\n",
        "        print('update learning rate: %f -> %f' % (self.old_lr, new_lr), file=mylog)\n",
        "        print('update learning rate: %f -> %f' % (self.old_lr, new_lr))\n",
        "        self.old_lr = new_lr"
      ],
      "metadata": {
        "id": "d8m-UoBQoY1D"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def randomHueSaturationValue(image, hue_shift_limit=(-180, 180),\n",
        "                             sat_shift_limit=(-255, 255),\n",
        "                             val_shift_limit=(-255, 255), u=0.5):\n",
        "    if np.random.random() < u:\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
        "        h, s, v = cv2.split(image)\n",
        "        hue_shift = np.random.randint(hue_shift_limit[0], hue_shift_limit[1]+1)\n",
        "        hue_shift = np.uint8(hue_shift)\n",
        "        h += hue_shift\n",
        "        sat_shift = np.random.uniform(sat_shift_limit[0], sat_shift_limit[1])\n",
        "        s = cv2.add(s, sat_shift)\n",
        "        val_shift = np.random.uniform(val_shift_limit[0], val_shift_limit[1])\n",
        "        v = cv2.add(v, val_shift)\n",
        "        image = cv2.merge((h, s, v))\n",
        "        #image = cv2.merge((s, v))\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_HSV2BGR)\n",
        "\n",
        "    return image\n",
        "\n",
        "def randomShiftScaleRotate(image, mask,\n",
        "                           shift_limit=(-0.0, 0.0),\n",
        "                           scale_limit=(-0.0, 0.0),\n",
        "                           rotate_limit=(-0.0, 0.0), \n",
        "                           aspect_limit=(-0.0, 0.0),\n",
        "                           borderMode=cv2.BORDER_CONSTANT, u=0.5):\n",
        "    if np.random.random() < u:\n",
        "        height, width, channel = image.shape\n",
        "\n",
        "        angle = np.random.uniform(rotate_limit[0], rotate_limit[1])\n",
        "        scale = np.random.uniform(1 + scale_limit[0], 1 + scale_limit[1])\n",
        "        aspect = np.random.uniform(1 + aspect_limit[0], 1 + aspect_limit[1])\n",
        "        sx = scale * aspect / (aspect ** 0.5)\n",
        "        sy = scale / (aspect ** 0.5)\n",
        "        dx = round(np.random.uniform(shift_limit[0], shift_limit[1]) * width)\n",
        "        dy = round(np.random.uniform(shift_limit[0], shift_limit[1]) * height)\n",
        "\n",
        "        cc = np.math.cos(angle / 180 * np.math.pi) * sx\n",
        "        ss = np.math.sin(angle / 180 * np.math.pi) * sy\n",
        "        rotate_matrix = np.array([[cc, -ss], [ss, cc]])\n",
        "\n",
        "        box0 = np.array([[0, 0], [width, 0], [width, height], [0, height], ])\n",
        "        box1 = box0 - np.array([width / 2, height / 2])\n",
        "        box1 = np.dot(box1, rotate_matrix.T) + np.array([width / 2 + dx, height / 2 + dy])\n",
        "\n",
        "        box0 = box0.astype(np.float32)\n",
        "        box1 = box1.astype(np.float32)\n",
        "        mat = cv2.getPerspectiveTransform(box0, box1)\n",
        "        image = cv2.warpPerspective(image, mat, (width, height), flags=cv2.INTER_LINEAR, borderMode=borderMode,\n",
        "                                    borderValue=(\n",
        "                                        0, 0,\n",
        "                                        0,))\n",
        "        mask = cv2.warpPerspective(mask, mat, (width, height), flags=cv2.INTER_LINEAR, borderMode=borderMode,\n",
        "                                   borderValue=(\n",
        "                                       0, 0,\n",
        "                                       0,))\n",
        "\n",
        "    return image, mask\n",
        "\n",
        "def randomHorizontalFlip(image, mask, u=0.5):\n",
        "    if np.random.random() < u:\n",
        "        image = cv2.flip(image, 1)\n",
        "        mask = cv2.flip(mask, 1)\n",
        "\n",
        "    return image, mask\n",
        "\n",
        "def randomVerticleFlip(image, mask, u=0.5):\n",
        "    if np.random.random() < u:\n",
        "        image = cv2.flip(image, 0)\n",
        "        mask = cv2.flip(mask, 0)\n",
        "\n",
        "    return image, mask\n",
        "\n",
        "def randomRotate90(image, mask, u=0.5):\n",
        "    if np.random.random() < u:\n",
        "        image=np.rot90(image)\n",
        "        mask=np.rot90(mask)\n",
        "\n",
        "    return image, mask\n",
        "\n",
        "def default_loader(id, root):\n",
        "    img = cv2.imread(os.path.join(root, '{}_sat.jpg').format(id))\n",
        "    mask = cv2.imread(os.path.join(root, '{}_mask.png').format(id), cv2.IMREAD_GRAYSCALE)\n",
        "    img = randomHueSaturationValue(img,\n",
        "                                   hue_shift_limit=(-30, 30),\n",
        "                                   sat_shift_limit=(-5, 5),\n",
        "                                   val_shift_limit=(-15, 15))\n",
        "    \n",
        "    img, mask = randomShiftScaleRotate(img, mask,\n",
        "                                       shift_limit=(-0.1, 0.1),\n",
        "                                       scale_limit=(-0.1, 0.1),\n",
        "                                       aspect_limit=(-0.1, 0.1),\n",
        "                                       rotate_limit=(-0, 0))\n",
        "    img, mask = randomHorizontalFlip(img, mask)\n",
        "    img, mask = randomVerticleFlip(img, mask)\n",
        "    img, mask = randomRotate90(img, mask)\n",
        "    \n",
        "    mask = np.expand_dims(mask, axis=2)\n",
        "    img = np.array(img, np.float32).transpose(2,0,1)/255.0 * 3.2 - 1.6\n",
        "    mask = np.array(mask, np.float32).transpose(2,0,1)/255.0\n",
        "    mask[mask>=0.5] = 1\n",
        "    mask[mask<=0.5] = 0\n",
        "    #mask = abs(mask-1)\n",
        "    return img, mask\n",
        "\n",
        "\n",
        "class ImageFolder(data.Dataset):\n",
        "\n",
        "    def __init__(self, trainlist, root):\n",
        "        self.ids = trainlist\n",
        "        self.loader = default_loader\n",
        "        self.root = root\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        id = self.ids[index]\n",
        "        img, mask = self.loader(id, self.root)\n",
        "        img = torch.Tensor(img)\n",
        "        mask = torch.Tensor(mask)\n",
        "        return img, mask\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ids)"
      ],
      "metadata": {
        "id": "G8bgeUxAmYsQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class dice_bce_loss(nn.Module):\n",
        "    def __init__(self, batch=True):\n",
        "        super(dice_bce_loss, self).__init__()\n",
        "        self.batch = batch\n",
        "        self.bce_loss = nn.BCELoss()\n",
        "        \n",
        "    def soft_dice_coeff(self, y_true, y_pred):\n",
        "        smooth = 0.0  # may change\n",
        "        if self.batch:\n",
        "            i = torch.sum(y_true)\n",
        "            j = torch.sum(y_pred)\n",
        "            intersection = torch.sum(y_true * y_pred)\n",
        "        else:\n",
        "            i = y_true.sum(1).sum(1).sum(1)\n",
        "            j = y_pred.sum(1).sum(1).sum(1)\n",
        "            intersection = (y_true * y_pred).sum(1).sum(1).sum(1)\n",
        "        score = (2. * intersection + smooth) / (i + j + smooth)\n",
        "        #score = (intersection + smooth) / (i + j - intersection + smooth)#iou\n",
        "        return score.mean()\n",
        "\n",
        "    def soft_dice_loss(self, y_true, y_pred):\n",
        "        loss = 1 - self.soft_dice_coeff(y_true, y_pred)\n",
        "        return loss\n",
        "        \n",
        "    def __call__(self, y_pred, y_true):\n",
        "        a =  self.bce_loss(y_pred, y_true)\n",
        "        b =  self.soft_dice_loss(y_true, y_pred)\n",
        "        return a + b"
      ],
      "metadata": {
        "id": "CeAJfH7qmYZr"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nonlinearity = partial(F.relu,inplace=True)\n",
        "\n",
        "class Dblock_more_dilate(nn.Module):\n",
        "    def __init__(self,channel):\n",
        "        super(Dblock_more_dilate, self).__init__()\n",
        "        self.dilate1 = nn.Conv2d(channel, channel, kernel_size=3, dilation=1, padding=1)\n",
        "        self.dilate2 = nn.Conv2d(channel, channel, kernel_size=3, dilation=2, padding=2)\n",
        "        self.dilate3 = nn.Conv2d(channel, channel, kernel_size=3, dilation=4, padding=4)\n",
        "        self.dilate4 = nn.Conv2d(channel, channel, kernel_size=3, dilation=8, padding=8)\n",
        "        self.dilate5 = nn.Conv2d(channel, channel, kernel_size=3, dilation=16, padding=16)\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.zero_()\n",
        "                    \n",
        "    def forward(self, x):\n",
        "        dilate1_out = nonlinearity(self.dilate1(x))\n",
        "        dilate2_out = nonlinearity(self.dilate2(dilate1_out))\n",
        "        dilate3_out = nonlinearity(self.dilate3(dilate2_out))\n",
        "        dilate4_out = nonlinearity(self.dilate4(dilate3_out))\n",
        "        dilate5_out = nonlinearity(self.dilate5(dilate4_out))\n",
        "        out = x + dilate1_out + dilate2_out + dilate3_out + dilate4_out + dilate5_out\n",
        "        return out\n",
        "\n",
        "class Dblock(nn.Module):\n",
        "    def __init__(self,channel):\n",
        "        super(Dblock, self).__init__()\n",
        "        self.dilate1 = nn.Conv2d(channel, channel, kernel_size=3, dilation=1, padding=1)\n",
        "        self.dilate2 = nn.Conv2d(channel, channel, kernel_size=3, dilation=2, padding=2)\n",
        "        self.dilate3 = nn.Conv2d(channel, channel, kernel_size=3, dilation=4, padding=4)\n",
        "        self.dilate4 = nn.Conv2d(channel, channel, kernel_size=3, dilation=8, padding=8)\n",
        "        #self.dilate5 = nn.Conv2d(channel, channel, kernel_size=3, dilation=16, padding=16)\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.zero_()\n",
        "                    \n",
        "    def forward(self, x):\n",
        "        dilate1_out = nonlinearity(self.dilate1(x))\n",
        "        dilate2_out = nonlinearity(self.dilate2(dilate1_out))\n",
        "        dilate3_out = nonlinearity(self.dilate3(dilate2_out))\n",
        "        dilate4_out = nonlinearity(self.dilate4(dilate3_out))\n",
        "        #dilate5_out = nonlinearity(self.dilate5(dilate4_out))\n",
        "        out = x + dilate1_out + dilate2_out + dilate3_out + dilate4_out# + dilate5_out\n",
        "        return out\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, in_channels, n_filters):\n",
        "        super(DecoderBlock,self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, in_channels // 4, 1)\n",
        "        self.norm1 = nn.BatchNorm2d(in_channels // 4)\n",
        "        self.relu1 = nonlinearity\n",
        "\n",
        "        self.deconv2 = nn.ConvTranspose2d(in_channels // 4, in_channels // 4, 3, stride=2, padding=1, output_padding=1)\n",
        "        self.norm2 = nn.BatchNorm2d(in_channels // 4)\n",
        "        self.relu2 = nonlinearity\n",
        "\n",
        "        self.conv3 = nn.Conv2d(in_channels // 4, n_filters, 1)\n",
        "        self.norm3 = nn.BatchNorm2d(n_filters)\n",
        "        self.relu3 = nonlinearity\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.norm1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.deconv2(x)\n",
        "        x = self.norm2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.norm3(x)\n",
        "        x = self.relu3(x)\n",
        "        return x\n",
        "    \n",
        "class DinkNet34_less_pool(nn.Module):\n",
        "    def __init__(self, num_classes=1):\n",
        "        super(DinkNet34_more_dilate, self).__init__()\n",
        "\n",
        "        filters = [64, 128, 256, 512]\n",
        "        resnet = models.resnet34(pretrained=True)\n",
        "        \n",
        "        self.firstconv = resnet.conv1\n",
        "        self.firstbn = resnet.bn1\n",
        "        self.firstrelu = resnet.relu\n",
        "        self.firstmaxpool = resnet.maxpool\n",
        "        self.encoder1 = resnet.layer1\n",
        "        self.encoder2 = resnet.layer2\n",
        "        self.encoder3 = resnet.layer3\n",
        "        \n",
        "        self.dblock = Dblock_more_dilate(256)\n",
        "\n",
        "        self.decoder3 = DecoderBlock(filters[2], filters[1])\n",
        "        self.decoder2 = DecoderBlock(filters[1], filters[0])\n",
        "        self.decoder1 = DecoderBlock(filters[0], filters[0])\n",
        "\n",
        "        self.finaldeconv1 = nn.ConvTranspose2d(filters[0], 32, 4, 2, 1)\n",
        "        self.finalrelu1 = nonlinearity\n",
        "        self.finalconv2 = nn.Conv2d(32, 32, 3, padding=1)\n",
        "        self.finalrelu2 = nonlinearity\n",
        "        self.finalconv3 = nn.Conv2d(32, num_classes, 3, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        x = self.firstconv(x)\n",
        "        x = self.firstbn(x)\n",
        "        x = self.firstrelu(x)\n",
        "        x = self.firstmaxpool(x)\n",
        "        e1 = self.encoder1(x)\n",
        "        e2 = self.encoder2(e1)\n",
        "        e3 = self.encoder3(e2)\n",
        "        \n",
        "        #Center\n",
        "        e3 = self.dblock(e3)\n",
        "\n",
        "        # Decoder\n",
        "        d3 = self.decoder3(e3) + e2\n",
        "        d2 = self.decoder2(d3) + e1\n",
        "        d1 = self.decoder1(d2)\n",
        "\n",
        "        # Final Classification\n",
        "        out = self.finaldeconv1(d1)\n",
        "        out = self.finalrelu1(out)\n",
        "        out = self.finalconv2(out)\n",
        "        out = self.finalrelu2(out)\n",
        "        out = self.finalconv3(out)\n",
        "\n",
        "        return F.sigmoid(out)\n",
        "    \n",
        "class DinkNet34(nn.Module):\n",
        "    def __init__(self, num_classes=1, num_channels=3):\n",
        "        super(DinkNet34, self).__init__()\n",
        "\n",
        "        filters = [64, 128, 256, 512]\n",
        "        resnet = models.resnet34(pretrained=True)\n",
        "        self.firstconv = resnet.conv1\n",
        "        self.firstbn = resnet.bn1\n",
        "        self.firstrelu = resnet.relu\n",
        "        self.firstmaxpool = resnet.maxpool\n",
        "        self.encoder1 = resnet.layer1\n",
        "        self.encoder2 = resnet.layer2\n",
        "        self.encoder3 = resnet.layer3\n",
        "        self.encoder4 = resnet.layer4\n",
        "        \n",
        "        self.dblock = Dblock(512)\n",
        "\n",
        "        self.decoder4 = DecoderBlock(filters[3], filters[2])\n",
        "        self.decoder3 = DecoderBlock(filters[2], filters[1])\n",
        "        self.decoder2 = DecoderBlock(filters[1], filters[0])\n",
        "        self.decoder1 = DecoderBlock(filters[0], filters[0])\n",
        "\n",
        "        self.finaldeconv1 = nn.ConvTranspose2d(filters[0], 32, 4, 2, 1)\n",
        "        self.finalrelu1 = nonlinearity\n",
        "        self.finalconv2 = nn.Conv2d(32, 32, 3, padding=1)\n",
        "        self.finalrelu2 = nonlinearity\n",
        "        self.finalconv3 = nn.Conv2d(32, num_classes, 3, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        x = self.firstconv(x)\n",
        "        x = self.firstbn(x)\n",
        "        x = self.firstrelu(x)\n",
        "        x = self.firstmaxpool(x)\n",
        "        e1 = self.encoder1(x)\n",
        "        e2 = self.encoder2(e1)\n",
        "        e3 = self.encoder3(e2)\n",
        "        e4 = self.encoder4(e3)\n",
        "        \n",
        "        # Center\n",
        "        e4 = self.dblock(e4)\n",
        "\n",
        "        # Decoder\n",
        "        d4 = self.decoder4(e4) + e3\n",
        "        d3 = self.decoder3(d4) + e2\n",
        "        d2 = self.decoder2(d3) + e1\n",
        "        d1 = self.decoder1(d2)\n",
        "        \n",
        "        out = self.finaldeconv1(d1)\n",
        "        out = self.finalrelu1(out)\n",
        "        out = self.finalconv2(out)\n",
        "        out = self.finalrelu2(out)\n",
        "        out = self.finalconv3(out)\n",
        "\n",
        "        return F.sigmoid(out)\n",
        "\n",
        "class DinkNet50(nn.Module):\n",
        "    def __init__(self, num_classes=1):\n",
        "        super(DinkNet50, self).__init__()\n",
        "\n",
        "        filters = [256, 512, 1024, 2048]\n",
        "        resnet = models.resnet50(pretrained=True)\n",
        "        self.firstconv = resnet.conv1\n",
        "        self.firstbn = resnet.bn1\n",
        "        self.firstrelu = resnet.relu\n",
        "        self.firstmaxpool = resnet.maxpool\n",
        "        self.encoder1 = resnet.layer1\n",
        "        self.encoder2 = resnet.layer2\n",
        "        self.encoder3 = resnet.layer3\n",
        "        self.encoder4 = resnet.layer4\n",
        "        \n",
        "        self.dblock = Dblock_more_dilate(2048)\n",
        "\n",
        "        self.decoder4 = DecoderBlock(filters[3], filters[2])\n",
        "        self.decoder3 = DecoderBlock(filters[2], filters[1])\n",
        "        self.decoder2 = DecoderBlock(filters[1], filters[0])\n",
        "        self.decoder1 = DecoderBlock(filters[0], filters[0])\n",
        "\n",
        "        self.finaldeconv1 = nn.ConvTranspose2d(filters[0], 32, 4, 2, 1)\n",
        "        self.finalrelu1 = nonlinearity\n",
        "        self.finalconv2 = nn.Conv2d(32, 32, 3, padding=1)\n",
        "        self.finalrelu2 = nonlinearity\n",
        "        self.finalconv3 = nn.Conv2d(32, num_classes, 3, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        x = self.firstconv(x)\n",
        "        x = self.firstbn(x)\n",
        "        x = self.firstrelu(x)\n",
        "        x = self.firstmaxpool(x)\n",
        "        e1 = self.encoder1(x)\n",
        "        e2 = self.encoder2(e1)\n",
        "        e3 = self.encoder3(e2)\n",
        "        e4 = self.encoder4(e3)\n",
        "        \n",
        "        # Center\n",
        "        e4 = self.dblock(e4)\n",
        "\n",
        "        # Decoder\n",
        "        d4 = self.decoder4(e4) + e3\n",
        "        d3 = self.decoder3(d4) + e2\n",
        "        d2 = self.decoder2(d3) + e1\n",
        "        d1 = self.decoder1(d2)\n",
        "        out = self.finaldeconv1(d1)\n",
        "        out = self.finalrelu1(out)\n",
        "        out = self.finalconv2(out)\n",
        "        out = self.finalrelu2(out)\n",
        "        out = self.finalconv3(out)\n",
        "\n",
        "        return F.sigmoid(out)\n",
        "    \n",
        "class DinkNet101(nn.Module):\n",
        "    def __init__(self, num_classes=1):\n",
        "        super(DinkNet101, self).__init__()\n",
        "\n",
        "        filters = [256, 512, 1024, 2048]\n",
        "        resnet = models.resnet101(pretrained=True)\n",
        "        self.firstconv = resnet.conv1\n",
        "        self.firstbn = resnet.bn1\n",
        "        self.firstrelu = resnet.relu\n",
        "        self.firstmaxpool = resnet.maxpool\n",
        "        self.encoder1 = resnet.layer1\n",
        "        self.encoder2 = resnet.layer2\n",
        "        self.encoder3 = resnet.layer3\n",
        "        self.encoder4 = resnet.layer4\n",
        "        \n",
        "        self.dblock = Dblock_more_dilate(2048)\n",
        "\n",
        "        self.decoder4 = DecoderBlock(filters[3], filters[2])\n",
        "        self.decoder3 = DecoderBlock(filters[2], filters[1])\n",
        "        self.decoder2 = DecoderBlock(filters[1], filters[0])\n",
        "        self.decoder1 = DecoderBlock(filters[0], filters[0])\n",
        "\n",
        "        self.finaldeconv1 = nn.ConvTranspose2d(filters[0], 32, 4, 2, 1)\n",
        "        self.finalrelu1 = nonlinearity\n",
        "        self.finalconv2 = nn.Conv2d(32, 32, 3, padding=1)\n",
        "        self.finalrelu2 = nonlinearity\n",
        "        self.finalconv3 = nn.Conv2d(32, num_classes, 3, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        x = self.firstconv(x)\n",
        "        x = self.firstbn(x)\n",
        "        x = self.firstrelu(x)\n",
        "        x = self.firstmaxpool(x)\n",
        "        e1 = self.encoder1(x)\n",
        "        e2 = self.encoder2(e1)\n",
        "        e3 = self.encoder3(e2)\n",
        "        e4 = self.encoder4(e3)\n",
        "        \n",
        "        # Center\n",
        "        e4 = self.dblock(e4)\n",
        "\n",
        "        # Decoder\n",
        "        d4 = self.decoder4(e4) + e3\n",
        "        d3 = self.decoder3(d4) + e2\n",
        "        d2 = self.decoder2(d3) + e1\n",
        "        d1 = self.decoder1(d2)\n",
        "        out = self.finaldeconv1(d1)\n",
        "        out = self.finalrelu1(out)\n",
        "        out = self.finalconv2(out)\n",
        "        out = self.finalrelu2(out)\n",
        "        out = self.finalconv3(out)\n",
        "\n",
        "        return F.sigmoid(out)\n",
        "\n",
        "class LinkNet34(nn.Module):\n",
        "    def __init__(self, num_classes=1):\n",
        "        super(LinkNet34, self).__init__()\n",
        "\n",
        "        filters = [64, 128, 256, 512]\n",
        "        resnet = models.resnet34(pretrained=True)\n",
        "        self.firstconv = resnet.conv1\n",
        "        self.firstbn = resnet.bn1\n",
        "        self.firstrelu = resnet.relu\n",
        "        self.firstmaxpool = resnet.maxpool\n",
        "        self.encoder1 = resnet.layer1\n",
        "        self.encoder2 = resnet.layer2\n",
        "        self.encoder3 = resnet.layer3\n",
        "        self.encoder4 = resnet.layer4\n",
        "\n",
        "        self.decoder4 = DecoderBlock(filters[3], filters[2])\n",
        "        self.decoder3 = DecoderBlock(filters[2], filters[1])\n",
        "        self.decoder2 = DecoderBlock(filters[1], filters[0])\n",
        "        self.decoder1 = DecoderBlock(filters[0], filters[0])\n",
        "\n",
        "        self.finaldeconv1 = nn.ConvTranspose2d(filters[0], 32, 3, stride=2)\n",
        "        self.finalrelu1 = nonlinearity\n",
        "        self.finalconv2 = nn.Conv2d(32, 32, 3)\n",
        "        self.finalrelu2 = nonlinearity\n",
        "        self.finalconv3 = nn.Conv2d(32, num_classes, 2, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        x = self.firstconv(x)\n",
        "        x = self.firstbn(x)\n",
        "        x = self.firstrelu(x)\n",
        "        x = self.firstmaxpool(x)\n",
        "        e1 = self.encoder1(x)\n",
        "        e2 = self.encoder2(e1)\n",
        "        e3 = self.encoder3(e2)\n",
        "        e4 = self.encoder4(e3)\n",
        "\n",
        "        # Decoder\n",
        "        d4 = self.decoder4(e4) + e3\n",
        "        d3 = self.decoder3(d4) + e2\n",
        "        d2 = self.decoder2(d3) + e1\n",
        "        d1 = self.decoder1(d2)\n",
        "        out = self.finaldeconv1(d1)\n",
        "        out = self.finalrelu1(out)\n",
        "        out = self.finalconv2(out)\n",
        "        out = self.finalrelu2(out)\n",
        "        out = self.finalconv3(out)\n",
        "\n",
        "        return F.sigmoid(out)"
      ],
      "metadata": {
        "id": "FFe1pEob40_U"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SHAPE = (1024,1024)\n",
        "ROOT = '/content/drive/MyDrive/Caps/train'\n",
        "imagelist = filter(lambda x: x.find('sat')!=-1, os.listdir(ROOT))\n",
        "trainlist = list(map(lambda x: x[:-8], imagelist))\n",
        "NAME = 'log-dlink34'\n",
        "LOG_DIR = '/content/drive/MyDrive/Caps/logs'\n",
        "if not os.path.exists(LOG_DIR):\n",
        "    os.makedirs(LOG_DIR)\n",
        "BATCHSIZE_PER_CARD = 2\n",
        "\n",
        "solver = MyFrame(DinkNet34, dice_bce_loss, 2e-4)\n",
        "batchsize = torch.cuda.device_count() * BATCHSIZE_PER_CARD\n",
        "\n",
        "dataset = ImageFolder(trainlist, ROOT)\n",
        "data_loader = torch.utils.data.DataLoader(\n",
        "    dataset,\n",
        "    batch_size=batchsize,\n",
        "    shuffle=True,\n",
        "    num_workers=4)\n",
        "\n",
        "mylog = open(os.path.join(LOG_DIR, NAME+'.log'), 'w')\n",
        "tic = time()\n",
        "no_optim = 0\n",
        "total_epoch = 35\n",
        "train_epoch_best_loss = 100.\n",
        "for epoch in range(1, total_epoch + 1):\n",
        "    data_loader_iter = iter(data_loader)\n",
        "    train_epoch_loss = 0\n",
        "    for img, mask in data_loader_iter:\n",
        "        solver.set_input(img, mask)\n",
        "        train_loss = solver.optimize()\n",
        "        train_epoch_loss += train_loss\n",
        "    train_epoch_loss /= len(data_loader_iter)\n",
        "    print('********', file=mylog)\n",
        "    print('epoch:', epoch, '    time:', int(time() - tic), file=mylog)\n",
        "    print('train_loss:', train_epoch_loss, file=mylog)\n",
        "    print('SHAPE:', SHAPE, file=mylog)\n",
        "    print('********', file=mylog)\n",
        "    print('epoch:', epoch, '    time:', int(time()-tic))\n",
        "    print('train_loss:',train_epoch_loss)\n",
        "    print('SHAPE:',SHAPE)\n",
        "    \n",
        "    if train_epoch_loss >= train_epoch_best_loss:\n",
        "        no_optim += 1\n",
        "    else:\n",
        "        no_optim = 0\n",
        "        train_epoch_best_loss = train_epoch_loss\n",
        "        solver.save('/content/drive/MyDrive/Caps'+NAME+'.th')\n",
        "    if no_optim > 6:\n",
        "        print('early stop at %d epoch' % epoch, file=mylog)\n",
        "        print('early stop at %d epoch' % epoch)\n",
        "        break\n",
        "    if no_optim > 3:\n",
        "        if solver.old_lr < 5e-7:\n",
        "            break\n",
        "        solver.load('/content/drive/MyDrive/Capslog-dlink34.th')\n",
        "        solver.update_lr(5.0, factor = True, mylog = mylog)\n",
        "    mylog.flush()\n",
        "    \n",
        "print('text', file=mylog)\n",
        "print('Finish!')\n",
        "mylog.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "455525c7c52748dcac1989af5ad1117d",
            "2f0a9397a450448d8a1ef142c84ea99b",
            "bf6d5d7d47ea4bde9fcacbd3a0d2bf3a",
            "4f793702741141bf845bad72c9d9523a",
            "1110f27d417247ab973ecb972350efc7",
            "c574b5962a3449acb49c9c3e3e5e8dcb",
            "72431c37d2a6486e9955ff17b4d99b05",
            "00fd2b48028c4d1a95fcd8cb666fe749",
            "008de073eeeb49828b98083074ed8009",
            "5a6384b1a5ba4ca4a1a11e0f4dca706a",
            "e64249df81f14bdab2b83ec66c7b3dd8"
          ]
        },
        "id": "HUllOkK3408u",
        "outputId": "21bf106c-ea90-49a0-e14e-5921b634fdf5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/83.3M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "455525c7c52748dcac1989af5ad1117d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.9/dist-packages/torch/nn/functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 1     time: 30\n",
            "train_loss: 1.2330376648902892\n",
            "SHAPE: (1024, 1024)\n",
            "epoch: 2     time: 54\n",
            "train_loss: 0.8399766278266907\n",
            "SHAPE: (1024, 1024)\n",
            "epoch: 3     time: 76\n",
            "train_loss: 0.7464782655239105\n",
            "SHAPE: (1024, 1024)\n",
            "epoch: 4     time: 98\n",
            "train_loss: 0.7142608428001403\n",
            "SHAPE: (1024, 1024)\n",
            "epoch: 5     time: 121\n",
            "train_loss: 0.6908887457847596\n",
            "SHAPE: (1024, 1024)\n",
            "epoch: 6     time: 143\n",
            "train_loss: 0.6726018691062927\n",
            "SHAPE: (1024, 1024)\n",
            "epoch: 7     time: 166\n",
            "train_loss: 0.5491017580032349\n",
            "SHAPE: (1024, 1024)\n",
            "epoch: 8     time: 190\n",
            "train_loss: 0.5377653050422668\n",
            "SHAPE: (1024, 1024)\n",
            "epoch: 9     time: 213\n",
            "train_loss: 0.5195117723941803\n",
            "SHAPE: (1024, 1024)\n",
            "epoch: 10     time: 235\n",
            "train_loss: 0.5008347803354263\n",
            "SHAPE: (1024, 1024)\n",
            "epoch: 11     time: 258\n",
            "train_loss: 0.473371599316597\n",
            "SHAPE: (1024, 1024)\n",
            "epoch: 12     time: 280\n",
            "train_loss: 0.507516912817955\n",
            "SHAPE: (1024, 1024)\n",
            "epoch: 13     time: 303\n",
            "train_loss: 0.45540692389011384\n",
            "SHAPE: (1024, 1024)\n",
            "epoch: 14     time: 326\n",
            "train_loss: 0.4498316866159439\n",
            "SHAPE: (1024, 1024)\n",
            "epoch: 15     time: 349\n",
            "train_loss: 0.4725466674566269\n",
            "SHAPE: (1024, 1024)\n",
            "epoch: 16     time: 371\n",
            "train_loss: 0.45701075851917267\n",
            "SHAPE: (1024, 1024)\n",
            "epoch: 17     time: 393\n",
            "train_loss: 0.42550859093666077\n",
            "SHAPE: (1024, 1024)\n",
            "epoch: 18     time: 416\n",
            "train_loss: 0.4379325085878372\n",
            "SHAPE: (1024, 1024)\n",
            "epoch: 19     time: 438\n",
            "train_loss: 0.4389278247952461\n",
            "SHAPE: (1024, 1024)\n",
            "epoch: 20     time: 461\n",
            "train_loss: 0.4263647228479385\n",
            "SHAPE: (1024, 1024)\n",
            "epoch: 21     time: 483\n",
            "train_loss: 0.40892390459775924\n",
            "SHAPE: (1024, 1024)\n",
            "epoch: 22     time: 506\n",
            "train_loss: 0.4057773733139038\n",
            "SHAPE: (1024, 1024)\n",
            "epoch: 23     time: 528\n",
            "train_loss: 0.4038621774315834\n",
            "SHAPE: (1024, 1024)\n",
            "epoch: 24     time: 551\n",
            "train_loss: 0.41100010603666304\n",
            "SHAPE: (1024, 1024)\n",
            "epoch: 25     time: 573\n",
            "train_loss: 0.3905380997061729\n",
            "SHAPE: (1024, 1024)\n",
            "epoch: 26     time: 597\n",
            "train_loss: 0.38318349719047545\n",
            "SHAPE: (1024, 1024)\n",
            "epoch: 27     time: 619\n",
            "train_loss: 0.38689220607280733\n",
            "SHAPE: (1024, 1024)\n",
            "epoch: 28     time: 641\n",
            "train_loss: 0.3964217540621757\n",
            "SHAPE: (1024, 1024)\n",
            "epoch: 29     time: 663\n",
            "train_loss: 0.4108834433555603\n",
            "SHAPE: (1024, 1024)\n",
            "epoch: 30     time: 685\n",
            "train_loss: 0.39008560866117475\n",
            "SHAPE: (1024, 1024)\n",
            "update learning rate: 0.000200 -> 0.000040\n",
            "epoch: 31     time: 708\n",
            "train_loss: 0.36379554465413094\n",
            "SHAPE: (1024, 1024)\n",
            "epoch: 32     time: 730\n",
            "train_loss: 0.3490962216258049\n",
            "SHAPE: (1024, 1024)\n",
            "epoch: 33     time: 753\n",
            "train_loss: 0.3414294943213463\n",
            "SHAPE: (1024, 1024)\n",
            "epoch: 34     time: 776\n",
            "train_loss: 0.3401492127776146\n",
            "SHAPE: (1024, 1024)\n",
            "epoch: 35     time: 799\n",
            "train_loss: 0.33817984491586683\n",
            "SHAPE: (1024, 1024)\n",
            "Finish!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCHSIZE_PER_CARD = 2\n",
        "\n",
        "class TTAFrame():\n",
        "    def __init__(self, net):\n",
        "        self.net = net().cuda()\n",
        "        self.net = torch.nn.DataParallel(self.net, device_ids=range(torch.cuda.device_count()))\n",
        "        \n",
        "    def test_one_img_from_path(self, path, evalmode = True):\n",
        "        if evalmode:\n",
        "            self.net.eval()\n",
        "        batchsize = torch.cuda.device_count() * BATCHSIZE_PER_CARD\n",
        "        if batchsize >= 8:\n",
        "            return self.test_one_img_from_path_1(path)\n",
        "        elif batchsize >= 4:\n",
        "            return self.test_one_img_from_path_2(path)\n",
        "        elif batchsize >= 2:\n",
        "            return self.test_one_img_from_path_4(path)\n",
        "\n",
        "    def test_one_img_from_path_8(self, path):\n",
        "        img = cv2.imread(path)#.transpose(2,0,1)[None]\n",
        "        img90 = np.array(np.rot90(img))\n",
        "        img1 = np.concatenate([img[None],img90[None]])\n",
        "        img2 = np.array(img1)[:,::-1]\n",
        "        img3 = np.array(img1)[:,:,::-1]\n",
        "        img4 = np.array(img2)[:,:,::-1]\n",
        "        \n",
        "        img1 = img1.transpose(0,3,1,2)\n",
        "        img2 = img2.transpose(0,3,1,2)\n",
        "        img3 = img3.transpose(0,3,1,2)\n",
        "        img4 = img4.transpose(0,3,1,2)\n",
        "        \n",
        "        img1 = V(torch.Tensor(np.array(img1, np.float32)/255.0 * 3.2 -1.6).cuda())\n",
        "        img2 = V(torch.Tensor(np.array(img2, np.float32)/255.0 * 3.2 -1.6).cuda())\n",
        "        img3 = V(torch.Tensor(np.array(img3, np.float32)/255.0 * 3.2 -1.6).cuda())\n",
        "        img4 = V(torch.Tensor(np.array(img4, np.float32)/255.0 * 3.2 -1.6).cuda())\n",
        "        \n",
        "        maska = self.net.forward(img1).squeeze().cpu().data.numpy()\n",
        "        maskb = self.net.forward(img2).squeeze().cpu().data.numpy()\n",
        "        maskc = self.net.forward(img3).squeeze().cpu().data.numpy()\n",
        "        maskd = self.net.forward(img4).squeeze().cpu().data.numpy()\n",
        "        \n",
        "        mask1 = maska + maskb[:,::-1] + maskc[:,:,::-1] + maskd[:,::-1,::-1]\n",
        "        mask2 = mask1[0] + np.rot90(mask1[1])[::-1,::-1]\n",
        "        \n",
        "        return mask2\n",
        "\n",
        "    def test_one_img_from_path_4(self, path):\n",
        "        img = cv2.imread(path)#.transpose(2,0,1)[None]\n",
        "        img90 = np.array(np.rot90(img))\n",
        "        img1 = np.concatenate([img[None],img90[None]])\n",
        "        img2 = np.array(img1)[:,::-1]\n",
        "        img3 = np.array(img1)[:,:,::-1]\n",
        "        img4 = np.array(img2)[:,:,::-1]\n",
        "        \n",
        "        img1 = img1.transpose(0,3,1,2)\n",
        "        img2 = img2.transpose(0,3,1,2)\n",
        "        img3 = img3.transpose(0,3,1,2)\n",
        "        img4 = img4.transpose(0,3,1,2)\n",
        "        \n",
        "        img1 = V(torch.Tensor(np.array(img1, np.float32)/255.0 * 3.2 -1.6).cuda())\n",
        "        img2 = V(torch.Tensor(np.array(img2, np.float32)/255.0 * 3.2 -1.6).cuda())\n",
        "        img3 = V(torch.Tensor(np.array(img3, np.float32)/255.0 * 3.2 -1.6).cuda())\n",
        "        img4 = V(torch.Tensor(np.array(img4, np.float32)/255.0 * 3.2 -1.6).cuda())\n",
        "        \n",
        "        maska = self.net.forward(img1).squeeze().cpu().data.numpy()\n",
        "        maskb = self.net.forward(img2).squeeze().cpu().data.numpy()\n",
        "        maskc = self.net.forward(img3).squeeze().cpu().data.numpy()\n",
        "        maskd = self.net.forward(img4).squeeze().cpu().data.numpy()\n",
        "        \n",
        "        mask1 = maska + maskb[:,::-1] + maskc[:,:,::-1] + maskd[:,::-1,::-1]\n",
        "        mask2 = mask1[0] + np.rot90(mask1[1])[::-1,::-1]\n",
        "        \n",
        "        return mask2\n",
        "    \n",
        "    def test_one_img_from_path_2(self, path):\n",
        "        img = cv2.imread(path)#.transpose(2,0,1)[None]\n",
        "        img90 = np.array(np.rot90(img))\n",
        "        img1 = np.concatenate([img[None],img90[None]])\n",
        "        img2 = np.array(img1)[:,::-1]\n",
        "        img3 = np.concatenate([img1,img2])\n",
        "        img4 = np.array(img3)[:,:,::-1]\n",
        "        img5 = img3.transpose(0,3,1,2)\n",
        "        img5 = np.array(img5, np.float32)/255.0 * 3.2 -1.6\n",
        "        img5 = V(torch.Tensor(img5).cuda())\n",
        "        img6 = img4.transpose(0,3,1,2)\n",
        "        img6 = np.array(img6, np.float32)/255.0 * 3.2 -1.6\n",
        "        img6 = V(torch.Tensor(img6).cuda())\n",
        "        \n",
        "        maska = self.net.forward(img5).squeeze().cpu().data.numpy()#.squeeze(1)\n",
        "        maskb = self.net.forward(img6).squeeze().cpu().data.numpy()\n",
        "        \n",
        "        mask1 = maska + maskb[:,:,::-1]\n",
        "        mask2 = mask1[:2] + mask1[2:,::-1]\n",
        "        mask3 = mask2[0] + np.rot90(mask2[1])[::-1,::-1]\n",
        "        \n",
        "        return mask3\n",
        "    \n",
        "    def test_one_img_from_path_1(self, path):\n",
        "        img = cv2.imread(path)#.transpose(2,0,1)[None]\n",
        "        \n",
        "        img90 = np.array(np.rot90(img))\n",
        "        img1 = np.concatenate([img[None],img90[None]])\n",
        "        img2 = np.array(img1)[:,::-1]\n",
        "        img3 = np.concatenate([img1,img2])\n",
        "        img4 = np.array(img3)[:,:,::-1]\n",
        "        img5 = np.concatenate([img3,img4]).transpose(0,3,1,2)\n",
        "        img5 = np.array(img5, np.float32)/255.0 * 3.2 -1.6\n",
        "        img5 = V(torch.Tensor(img5).cuda())\n",
        "        \n",
        "        mask = self.net.forward(img5).squeeze().cpu().data.numpy()#.squeeze(1)\n",
        "        mask1 = mask[:4] + mask[4:,:,::-1]\n",
        "        mask2 = mask1[:2] + mask1[2:,::-1]\n",
        "        mask3 = mask2[0] + np.rot90(mask2[1])[::-1,::-1]\n",
        "        \n",
        "        return mask3\n",
        "\n",
        "    def load(self, path):\n",
        "        self.net.load_state_dict(torch.load(path))\n",
        "        \n",
        "#source = 'dataset/test/'\n",
        "source = '/content/drive/MyDrive/Caps/val'\n",
        "val = os.listdir(source)\n",
        "solver = TTAFrame(DinkNet34)\n",
        "solver.load('/content/drive/MyDrive/Capslog-dlink34.th')\n",
        "tic = time()\n",
        "target = '/content/drive/MyDrive/Caps/log05_dink34'\n",
        "# os.mkdir(target)\n",
        "for i,name in enumerate(val):\n",
        "    if i%10 == 0:\n",
        "        print(i/10, '    ','%.2f'%(time()-tic))\n",
        "    mask = solver.test_one_img_from_path(source+name)\n",
        "    mask[mask>4.0] = 255\n",
        "    mask[mask<=4.0] = 0\n",
        "    mask = np.concatenate([mask[:,:,None],mask[:,:,None],mask[:,:,None]],axis=2)\n",
        "    cv2.imwrite(target+name[:-7]+'mask.png',mask.astype(np.uint8))"
      ],
      "metadata": {
        "id": "uqsL9DAp402t",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508
        },
        "outputId": "1e267f2f-5e1c-47d4-adea-62bc746d43b5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0      0.00\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-f839d009e3bc>\u001b[0m in \u001b[0;36m<cell line: 128>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'    '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'%.2f'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_one_img_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m     \u001b[0mmask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m4.0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0mmask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m<=\u001b[0m\u001b[0;36m4.0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-f839d009e3bc>\u001b[0m in \u001b[0;36mtest_one_img_from_path\u001b[0;34m(self, path, evalmode)\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_one_img_from_path_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mbatchsize\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_one_img_from_path_4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtest_one_img_from_path_8\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-f839d009e3bc>\u001b[0m in \u001b[0;36mtest_one_img_from_path_4\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtest_one_img_from_path_4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#.transpose(2,0,1)[None]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mimg90\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrot90\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0mimg1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimg90\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mimg2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36mrot90\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36mrot90\u001b[0;34m(m, k, axes)\u001b[0m\n\u001b[1;32m    225\u001b[0m     if (axes[0] >= m.ndim or axes[0] < -m.ndim\n\u001b[1;32m    226\u001b[0m         or axes[1] >= m.ndim or axes[1] < -m.ndim):\n\u001b[0;32m--> 227\u001b[0;31m         raise ValueError(\"Axes={} out of range for array of ndim={}.\"\n\u001b[0m\u001b[1;32m    228\u001b[0m             .format(axes, m.ndim))\n\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Axes=(0, 1) out of range for array of ndim=0."
          ]
        }
      ]
    }
  ]
}